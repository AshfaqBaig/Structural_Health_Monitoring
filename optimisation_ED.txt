Edge agent
piling up of images
triton server

nvtop

docker stats

nvidia-smi -l 1

nproc --all (Gives the number of cores of the system)

Visualisation of the triton server performance metric  

Benchmarking for different batch sizes for triton inference.

Triton inference performnace metrics 

edge agent is routing the messages to modules need to check
https://docs.microsoft.com/en-us/azure/iot-edge/how-to-access-built-in-metrics?view=iotedge-2020-11


Triton server optimisation

1.Batching
2.Model optimisation 
3. model wramup 
4. Dynamic batching
5. preserving the order

1. visualise the logs in azure iothub
2.onenote
3. debug log mqqt 

Agenda:
1. How to capture the time for routing the between the modules
2. How to parse the logs to iot hub and visualise it in dashboard.
3. How to copy the container logs.

Agenda:
Acurracy of decision module and edge verification, edge detection 

Running docker in dkaldev:tensorRT
docker run --gpus all -it --rm -v /home/bashfaq/Code/Model_optimisation:/workspace nvcr.io/nvidia/tensorrt:21.06-py3

pip install tensorflow-gpu

Running docker in edge2:tensorRT

docker run --gpus all -it --rm -v /home/bashfaq/code/Model_optimisation:/workspace nvcr.io/nvidia/tensorrt:20.08-py3

Running docker for tensorflow container

docker  run --gpus all -it --rm -v /home/bashfaq/Code/Model_optimisation:/workspace nvcr.io/nvidia/tensorflow:21.06-tf1-py3

Edge1: convertion of saved model to onnx

docker run --gpus all -it --rm -v /home/bashfaq/code/Model_optimisation:/workspace nvcr.io/nvidia/tensorrt:21.07-py3
pip install -U tf2onnx
pip install onnxmltools
pip install tensorflow-gpu
pip3 install onnx==1.8.0
pip install onnxruntime-gpu==1.7.0
python -m tf2onnx.convert --saved-model ./model.savedmodel --opset 13 --output model.onnx --verbose 
python3 -m pip install onnx_graphsurgeon --index-url https://pypi.ngc.nvidia.com
pip3 install -r requirement.txt

sudo apt-get install -y --no-install-recommends libnvinfer7=7.1.3-1+cuda11.4 \
    libnvinfer-dev=7.1.3-1+cuda11.4 \
    libnvinfer-plugin7=7.1.3-1+cuda11.4

wget https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64/libnvinfer7_7.1.3-1+cuda11.3_amd64.deb
sudo apt install ./libnvinfer7_7.1.3-1+cuda11.3_amd64.deb

Optical: conversion of onnx to tensorRT
docker run --gpus all -it --rm -v /home/bashfaq/code/DexiNed_Pytorch:/workspace nvcr.io/nvidia/tensorrt:21.08-py3

